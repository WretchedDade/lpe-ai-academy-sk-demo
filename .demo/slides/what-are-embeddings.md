---
theme: default
layout: default
---

# What are embeddings

In AI, **embeddings** are numerical representations of data—like words, sentences, images, or even entire documents—that capture their meaning, context, or relationships in a high-dimensional vector space. The goal of embeddings is to translate complex, unstructured data into a format that machines can understand and compare.

For example:

- In **natural language processing (NLP)**, word embeddings (like Word2Vec, GloVe, or those from transformer models) place similar words close together in vector space (e.g., “king” and “queen”).
- In **Semantic Kernel**, embeddings are used to store and retrieve relevant memories or documents based on semantic similarity, enabling context-aware AI agents.
They’re essential for tasks like search, recommendation, clustering, and reasoning.